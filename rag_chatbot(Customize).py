# -*- coding: utf-8 -*-
"""rag_langchain_huggingface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifL9PuS061ytUIErUN5Wke0yUsjmqZwY

# **RAG Application** with LangChain and HuggingFace LLM
"""

# Install the necessary packages
!pip install torch -q
!pip install transformers -q
!pip install numpy -q
!pip install langchain -q
!pip install langchain_community -q
!pip install langchain-chroma -q
!pip install sentence_transformers -q

import os
from google.colab import userdata

"""### Initialize HuggingFace LLM

Model repo url: https://huggingface.co/mistralai/Mistral-7B-v0.1
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.llms import HuggingFacePipeline
import torch

# Define the model ID
model_id = "mistralai/Mistral-7B-v0.1"

# Get your API token
huggingface_api_token = "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"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id, token=huggingface_api_token)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16, token=huggingface_api_token)

# Create a text generation pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.1, # Use temperature here as before
    trust_remote_code=True,
    device_map="auto",
)

# Initialize the HuggingFace llm using the pipeline
llm = HuggingFacePipeline(pipeline=pipe)

# Delete the model and tokenizer to free up GPU memory
del model
del tokenizer
del pipe
import gc
gc.collect()
torch.cuda.empty_cache()

"""### Initialize Embedding Model

Model url: https://sbert.net/
"""

from langchain_community.embeddings import HuggingFaceEmbeddings
import torch

# Ensure CUDA cache is empty before trying to load embedding model
torch.cuda.empty_cache()

embedding_model = HuggingFaceEmbeddings(
  model_name="sentence-transformers/all-mpnet-base-v2",
  model_kwargs={'device': 'cpu'} # Explicitly load the embedding model to CPU
)

"""### Initialize Output Parser"""

from langchain_core.output_parsers import StrOutputParser

output_parser=StrOutputParser()

"""### Load PDF Document"""

!pip install pypdf -qU

from langchain_community.document_loaders import PyPDFLoader

# Load the PDF document
loader = PyPDFLoader("/content/All in One Lectures - IT3 021-DWBI.pdf")

docs = loader.load()

len(docs)

docs[0]

"""### Split Documents into Chunks"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Initialize the text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)

# Split the documents into chunks
splits = text_splitter.split_documents(docs)

len(splits)

"""### Create Vector Store and Retriever"""

from langchain_chroma import Chroma

# Create a vector store from the document chunks
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)

# Create a retriever from the vector store
retriever = vectorstore.as_retriever()

"""### Define Prompt Template"""

from langchain_core.prompts import ChatPromptTemplate

# Define prompt template
template = """
Answer this question using the provided context only.

{question}

Context:
{context}

Answer:
"""

prompt=ChatPromptTemplate.from_template(template)

prompt

"""### Chain Retriever and Prompt Template with LLM"""

from langchain_core.runnables import RunnablePassthrough

chain = (
    {"context": retriever,  "question": RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

"""#### Invoke RAG Chain with Example Questions"""

response = chain.invoke("what is data warehousing?")
print(response)