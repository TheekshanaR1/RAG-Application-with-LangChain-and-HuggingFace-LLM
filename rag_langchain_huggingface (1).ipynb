{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPrQevtVHfAa"
   },
   "source": [
    "# **RAG Application** with LangChain and HuggingFace LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAz0poIYiyQF"
   },
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "!pip install torch -q\n",
    "!pip install transformers -q\n",
    "!pip install numpy -q\n",
    "!pip install langchain -q\n",
    "!pip install langchain_community -q\n",
    "!pip install langchain-chroma -q\n",
    "!pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvtHhNb7LlLe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imsRWg7LHnfa"
   },
   "source": [
    "### Initialize HuggingFace LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlJ6qI0xJBln"
   },
   "source": [
    "Model repo url: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8002fa3cecb5484b89771d9ddb181f49",
      "e097d3b7eea340d19acbd6b9d312b73f",
      "6dc70e04134a4d5f8c1250c950726b12",
      "07e8ff02635f428cbcf06db7118572ba",
      "db00af8adf704840b64ab9b58d9095d9",
      "1d5ee7fe082848089d657e9235368a0b",
      "8e1f0a5aa4d44d82a95ca618c1ca3ec2",
      "9f99b6e8520343429225df472f9cbb00",
      "cdc5456fe55d4ba49871463cd67b7adc",
      "8177979045424177a7744d2216d086ab",
      "74eb8774d7314511a5ac5a1ae6205574"
     ]
    },
    "id": "CQco8KHsLsoE",
    "outputId": "08e2ef04-3002-4cb3-ff99-25a39d1281af"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Get your API token\n",
    "huggingface_api_token = \"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\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=huggingface_api_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16, token=huggingface_api_token)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1, # Use temperature here as before\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Initialize the HuggingFace llm using the pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Delete the model and tokenizer to free up GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "del pipe\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY_2RLpfHq5q"
   },
   "source": [
    "### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHF9sDROJOYA"
   },
   "source": [
    "Model url: https://sbert.net/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "59092816661b4068b15fca18e18beff3",
      "77103d797b46412bb874e731382de49f",
      "3ee91f773fc84fba9e073c2e2b458303",
      "338d2751c5fc4635aa917421e16192b1",
      "1c64be92d10547e58fcf08e1503ff957",
      "65fa462c1b934bcbb9eac1aaa6528431",
      "63584505f34348b58567ec59d37d39a5",
      "4ffc6d44cc6d4b8ab8d6ac6a58b7f4ae",
      "520b875be1c44ecdb3b20f02b988def0",
      "f94e141e26114d688b3d78dc7e8a4754",
      "3ef819dbcd0e4cc3b6d13b1a71a236f1"
     ]
    },
    "id": "v5VIj7wtD85u",
    "outputId": "2b710923-2e66-4f9f-b01d-2d5f069d652f"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "# Ensure CUDA cache is empty before trying to load embedding model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "  model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "  model_kwargs={'device': 'cpu'} # Explicitly load the embedding model to CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIowHT1AIk6g"
   },
   "source": [
    "### Initialize Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ph6yJRAeI3Dg"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLPtLHuaHyDh"
   },
   "source": [
    "### Load PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtE4Pcb_ElWT"
   },
   "outputs": [],
   "source": [
    "!pip install pypdf -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj8NjIe9ElTX"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\"/content/2025-S1-IT3021-Lecture-01-Introduction.pdf\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JaKCnaeElQQ",
    "outputId": "7b3909b3-8121-4c88-f74a-5b61c2c9bb26"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** `27`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUpL3Z7BElNH",
    "outputId": "6f84d662-e011-4f84-de48-1d1afc682015"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output:**\n",
    "```\n",
    "Document(metadata={'producer': 'Microsoft® PowerPoint® for Microsoft 365', 'creator': 'Microsoft® PowerPoint® for Microsoft 365', 'creationdate': '2023-02-13T22:59:45+05:30', 'moddate': '2023-02-13T22:59:45+05:30', 'title': 'IT3021: Data Warehousing and Business Intelligence', 'source': '/content/2025-S1-IT3021-Lecture-01-Introduction.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}, page_content='BSc in IT: Specialising in Data Science\\nIT3021: Data Warehousing \\nand Business Intelligence\\nLecture 01\\nIntroduction to DW & BI')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K9dWgI7H2fA"
   },
   "source": [
    "### Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu4Ol5uEElKD"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "# Split the documents into chunks\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnX6N-TfE2x0",
    "outputId": "dd54778a-c3b7-4722-f264-befe9c46955a"
   },
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** `42` (document chunks after splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QXX_0zdH7qw"
   },
   "source": [
    "### Create Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N_28g2TElG7"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create a vector store from the document chunks\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFUj2lDfElEL"
   },
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Extract only the text from Document objects\n",
    "def docs_to_text(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Make a Runnable that converts list of Documents → string\n",
    "doc_text_runnable = RunnableLambda(docs_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeZ-Mdj1ICp4"
   },
   "source": [
    "### Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMlaWduIElBL"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrDIRXFXEk7D",
    "outputId": "1264d5ac-6cd1-44cc-d99b-35412efdd78d"
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DuaAR7YIUvo"
   },
   "source": [
    "### Chain Retriever and Prompt Template with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxdveKlzE8ML"
   },
   "outputs": [],
   "source": [
    "# Build the RAG chain with proper document formatting\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough.assign(context=lambda x: doc_text_runnable.invoke(x[\"context\"]))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:** `✓ RAG chain created successfully`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvP5N4nuIIwQ"
   },
   "source": [
    "#### Invoke RAG Chain with Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i59Ts0rWE8I7",
    "outputId": "5efc2b2e-5b00-4a74-b6c7-af0ead7ae4af"
   },
   "outputs": [],
   "source": [
    "question = \"what is data warehousing?\"\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Question: what is data warehousing?\n",
    "\n",
    "Answer:\n",
    "Data warehousing is a set of processes, architectures and technologies for collecting and managing data from various sources to support deriving meaningful business insights from raw data. Data collection involves data gathering, transforming and storing. It also includes database creation and data integration process development.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with another question\n",
    "question2 = \"What does data collection involve?\"\n",
    "response2 = chain.invoke(question2)\n",
    "\n",
    "print(\"Question:\", question2)\n",
    "print(\"\\nAnswer:\")\n",
    "print(response2.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Question: What does data collection involve?\n",
    "\n",
    "Answer:\n",
    "Data collection involves data gathering, transforming and storing. It includes the processes of collecting data from various sources, processing and organizing it for storage in a data warehouse.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
